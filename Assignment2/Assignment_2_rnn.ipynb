{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QXUa6oBtimfY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\argparse-1.4.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\collection-0.1.6-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\configparser-7.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\execnet-2.1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\faker-33.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\glob2-0.7-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\imageio-2.36.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\iniconfig-2.0.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\names-0.3.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\opencv_python-4.10.0.84-py3.13-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\os.path2-0.0.4-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pluggy-1.5.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pymysql-1.1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pytest-8.3.4-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pytest_shutil-1.8.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_abc-0.2.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_math-0.0.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_time-0.3.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\random2-1.0.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\shutils-0.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\sidtd-0.0.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\termcolor-2.5.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\typing-3.10.0.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\wget-3.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\argparse-1.4.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\collection-0.1.6-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\configparser-7.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\execnet-2.1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\faker-33.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\glob2-0.7-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\imageio-2.36.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\iniconfig-2.0.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\names-0.3.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\opencv_python-4.10.0.84-py3.13-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\os.path2-0.0.4-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pluggy-1.5.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pymysql-1.1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pytest-8.3.4-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\pytest_shutil-1.8.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_abc-0.2.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_math-0.0.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\python_time-0.3.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\random2-1.0.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\shutils-0.1.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\sidtd-0.0.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\termcolor-2.5.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\typing-3.10.0.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\wget-3.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKYp9jQPSGJQ"
   },
   "source": [
    "# Assignment 2: RNN and NER\n",
    "Welcome to the second assignment of CSE-476! In this project, you will be asked to implement a functional RNN using Pytorch, and then use it to do the same task as last time: NER. Hopefully, you will observe a much better perofermance with RNNs.\n",
    "\n",
    "## Task 1: Implement the RNN\n",
    "\n",
    "Your first task is to implement the missing part of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rq8DAh0YgiD5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        '''\n",
    "        TODO: Create all the necessary weights here.\n",
    "        Recall for RNN, you need\n",
    "        - a weight matrix from input to hidden (W)\n",
    "        - a weight matrix from hidden to hidden (U)\n",
    "        - a bias term b of size hidden, so that we have (Wx+Uh+b) before activation\n",
    "        '''\n",
    "        self.weights_ih = nn.Linear(self.input_dim, self.hidden_dim, bias=False)#nn.Parameter(torch.empty(self.input_dim, self.hidden_dim))\n",
    "        self.weights_hh = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)#nn.Parameter(torch.empty(self.hidden_dim, self.hidden_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "        # Weights defintions below\n",
    "\n",
    "        # Call weight initialization function\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        TODO: initialize the weights W/U with\n",
    "        https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\n",
    "        Note as this function takes a tensor, you need to pass in W.weights,\n",
    "        if you initialize W with nn.Linear().\n",
    "        '''\n",
    "        nn.init.kaiming_uniform_(self.weights_ih.weight)\n",
    "        nn.init.kaiming_uniform_(self.weights_hh.weight)\n",
    "\n",
    "    def forward(self, x_cur, hidden_prev):\n",
    "        '''\n",
    "        TODO: compute the forward pass using torch.tanh() as the activation function.\n",
    "        You should add the transformed input (size hidden_dim) and the transformed previous weights (size hidden_dim)\n",
    "        and the bias term b into a size hidden_dim vector, as the input to tanh().\n",
    "        i.e., (Wx+Uh+b)\n",
    "        Return the current hidden state.\n",
    "        '''\n",
    "        h_cur = torch.tanh(self.weights_ih(x_cur) + self.weights_hh(hidden_prev) + self.bias)\n",
    "        return h_cur\n",
    "\n",
    "\n",
    "# This class is a wrapper to the RNN cell to make it work on any output dims.\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_cell = SimpleRNNCell(input_dim, hidden_dim)\n",
    "        '''\n",
    "        TODO: you need to define some weights here to map the hidden size to output size\n",
    "        This is the V matrix as defined in the slides\n",
    "        '''\n",
    "        self.V = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The last part of x is the hidden size, which is not needed\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        '''\n",
    "        TODO: finish the implementation of the RNN forward call.\n",
    "        You need to\n",
    "        - 1) keep track of what if the current hidden state.\n",
    "        - 2) implement a for loop to acquire all the hidden states for each input\n",
    "        - 3) keep all the hidden states from each timestamp\n",
    "        - 4) compute the output y using V and the hidden states (note, you can do it for each timestamp, or together in one call)\n",
    "        Return all the predicted scores y_scores.\n",
    "        If you print y_scores.shape, it should be (batch_size, seq_len, output_dim)\n",
    "        Meaning that you are giving a output score/probability for all possible labels at all timestamp/positions, in all the sequences in the batch.\n",
    "        Hint: you may want to consider initializing first hidden state with zero values.\n",
    "        '''\n",
    "        cur_hid = torch.zeros(batch_size, self.hidden_dim)\n",
    "        states = list()\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            x_cur = x[:, i, :]\n",
    "            cur_hid = self.rnn_cell(x_cur, cur_hid)\n",
    "            states.append(cur_hid)\n",
    "            \n",
    "        states_stack = torch.stack(states, dim=1)\n",
    "        y_scores = self.V(states_stack)\n",
    "        \n",
    "        return y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "z4wGtJFOgvew"
   },
   "outputs": [],
   "source": [
    "# This is a wrapper class for using RNN on NER.\n",
    "# A minimum working version is given, but you can (and should) customize it to make the performances better.\n",
    "class SimpleRNNForNER(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128, output_dim=256, dropout=0.2, num_layers=3):\n",
    "        super(SimpleRNNForNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        # Note here that we are using \"hidden_dim\" as the output_dim for our RNN class,\n",
    "        # and then project that to the number of NER labels (tagset_size)\n",
    "        # Is it the best thing to do? Think about and play with it.\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        self.output_dim = hidden_dim*2\n",
    "        self.rnn = SimpleRNN(embedding_dim, hidden_dim, hidden_dim)\n",
    "        self.rnn_layers.append(self.rnn)\n",
    "\n",
    "        for i in range(num_layers-2):\n",
    "            embedding_dim = hidden_dim\n",
    "            #hidden_dim *= 2\n",
    "            #output_dim = hidden_dim*2\n",
    "            #self.rnn_layers.append(SimpleRNN(hidden_dim, hidden_dim, hidden_dim))\n",
    "            self.rnn_layers.append(SimpleRNN(embedding_dim, hidden_dim, hidden_dim))\n",
    "\n",
    "        #output_dim = hidden_dim*2\n",
    "        self.rnn_layers.append(SimpleRNN(hidden_dim, hidden_dim, output_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(output_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #x = self.embedding_dropout(x)\n",
    "\n",
    "        for i, rnn_layer in enumerate(self.rnn_layers):\n",
    "            x = rnn_layer(x)\n",
    "            #if i < len(self.rnn_layers) - 1:\n",
    "            #    x = self.dropout(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        tag_scores = self.fc(x)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUcxXvj1iBbD"
   },
   "source": [
    "## Task 2: Applying SimpleRNNForNER to NER.\n",
    "Congratulations of getting this far! Now we have the model for NER, let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eU63XwZuhL1-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# We are providing an implementation to load the NER dataset.\n",
    "# However, you should still read it over carefully to understand how it works.\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def extract_ner_data():\n",
    "    # Load the CoNLL 2003 dataset\n",
    "    datasets = load_dataset('conll2003', trust_remote_code=True)\n",
    "\n",
    "    # Load a pre-trained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "    # Tokenize and encode the datasets\n",
    "    def encode_tags(tags, encodings, tag2id):\n",
    "        labels = [[tag for tag in doc] for doc in tags]\n",
    "        encoded_labels = []\n",
    "        for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "            # create an empty array of -100\n",
    "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100\n",
    "            arr_offset = np.array(doc_offset)\n",
    "\n",
    "            # set labels whose first offset position is 0 and the second is not 0\n",
    "            doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "        return encoded_labels\n",
    "\n",
    "    tag2id = {tag: i for i, tag in enumerate(datasets['train'].features['ner_tags'].feature.names)}\n",
    "    id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "    # Tokenize the input sentences and encode labels\n",
    "    train_encodings = tokenizer(datasets['train']['tokens'], is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    train_labels = encode_tags(datasets['train']['ner_tags'], train_encodings, tag2id)\n",
    "\n",
    "    val_encodings = tokenizer(datasets['validation']['tokens'], is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    val_labels = encode_tags(datasets['validation']['ner_tags'], val_encodings, tag2id)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NERDataset(train_encodings, train_labels)\n",
    "    val_dataset = NERDataset(val_encodings, val_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, tag2id, id2tag\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, epochs=3, batch_size=32):\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    '''\n",
    "    TODO: You need to define the optimizer and the criterions here.\n",
    "    You should use\n",
    "    - torch.optim.Adam() as the optimizer\n",
    "    - nn.CrossEntropyLoss as the criterion\n",
    "    Hint: we don't want to penalize the model on padded labels (-100).\n",
    "    nn.CrossEntropyLoss has \"ignore_index\" argument that you can utilze.\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(params=model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    # Id to labels mapping\n",
    "    id_to_labels = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "            # inputs here should be in a ready-shape as input parameter to SimpleRNNForNER's forward() call.\n",
    "            # in other words, you should be able to call model(inputs) without errors if your model is correctly implemented.\n",
    "            inputs, labels = batch['input_ids'], batch['labels']\n",
    "            '''\n",
    "            TODO: implement the missing parts in this training loop.\n",
    "            - remember zero_grad()?\n",
    "            - remember backward()?\n",
    "            - remember step()?\n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "            res = model(inputs)\n",
    "            x, y, z = res.size()\n",
    "\n",
    "            res_tensor = res.view(-1, z)\n",
    "            labels_tensor = labels.view(-1)\n",
    "            \n",
    "            loss = criterion(res_tensor, labels_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # note this loss should be defined in your implementation above.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        '''\n",
    "        TODO: what do we need to do to put model in eval mode?\n",
    "        '''\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Evaluation\"):\n",
    "                inputs, labels = batch['input_ids'], batch['labels']\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=-1)\n",
    "                all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "                all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        '''\n",
    "        TODO: We are providing the F1 calculation for each label in the NER below.\n",
    "        However, when you are iterating the model design to improve the performances,\n",
    "        it is ideal to have a single number performance. To do this, you will implement\n",
    "        Macro-F1 score, which is simply the average of all F1 scores from all labels.\n",
    "        Return this score is encouraged, so you can observe improvements later.\n",
    "        '''\n",
    "        label_stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
    "\n",
    "        for true_label, pred_label in zip(all_labels, all_preds):\n",
    "            if true_label == -100:\n",
    "                continue  # Ignore padded tokens\n",
    "\n",
    "            for label_id in id_to_labels.keys():\n",
    "                if true_label == label_id:\n",
    "                    if pred_label == true_label:\n",
    "                        label_stats[label_id]['tp'] += 1\n",
    "                    else:\n",
    "                        label_stats[label_id]['fn'] += 1\n",
    "                elif pred_label == label_id:\n",
    "                    label_stats[label_id]['fp'] += 1\n",
    "        macro_f1 = list()\n",
    "        for label_id, stats in label_stats.items():\n",
    "            precision = stats['tp'] / (stats['tp'] + stats['fp']) if (stats['tp'] + stats['fp']) > 0 else 0.0\n",
    "            recall = stats['tp'] / (stats['tp'] + stats['fn']) if (stats['tp'] + stats['fn']) > 0 else 0.0\n",
    "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "            label_name = id_to_labels[label_id]\n",
    "            macro_f1.append(f1)\n",
    "            print(f\"Label '{label_name}': Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "        print(f\"Macro F1: {sum(macro_f1)/len(macro_f1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3r4F1jpkOTG"
   },
   "source": [
    "## Task 3: Test your NER model and improve\n",
    "\n",
    "You should now have a working NER model and training/eval pipeline. Your job in this task is to run the model, observe the performances, and make improvements. We first provide you with a simple example script to train and test your current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fwjAll1Iiz0_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:09<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 17.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.4760\n",
      "Label 'O': Precision: 0.8901, Recall: 0.9860, F1 Score: 0.9356\n",
      "Label 'B-ORG': Precision: 0.2915, Recall: 0.1872, F1 Score: 0.2280\n",
      "Label 'B-LOC': Precision: 0.5589, Recall: 0.4807, F1 Score: 0.5168\n",
      "Label 'B-MISC': Precision: 0.3133, Recall: 0.0510, F1 Score: 0.0877\n",
      "Label 'I-MISC': Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Label 'B-PER': Precision: 0.7988, Recall: 0.2845, F1 Score: 0.4195\n",
      "Label 'I-PER': Precision: 0.8439, Recall: 0.3351, F1 Score: 0.4797\n",
      "Label 'I-ORG': Precision: 0.3609, Recall: 0.0639, F1 Score: 0.1086\n",
      "Label 'I-LOC': Precision: 0.7010, Recall: 0.2646, F1 Score: 0.3842\n",
      "Macro F1: 0.3511279164074805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:05<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 19.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.3771\n",
      "Label 'O': Precision: 0.9403, Recall: 0.9772, F1 Score: 0.9584\n",
      "Label 'B-PER': Precision: 0.7357, Recall: 0.4245, F1 Score: 0.5384\n",
      "Label 'B-ORG': Precision: 0.3448, Recall: 0.3296, F1 Score: 0.3370\n",
      "Label 'I-ORG': Precision: 0.3034, Recall: 0.4407, F1 Score: 0.3594\n",
      "Label 'I-LOC': Precision: 0.4570, Recall: 0.5370, F1 Score: 0.4937\n",
      "Label 'B-LOC': Precision: 0.6673, Recall: 0.5732, F1 Score: 0.6167\n",
      "Label 'B-MISC': Precision: 0.5477, Recall: 0.2614, F1 Score: 0.3539\n",
      "Label 'I-MISC': Precision: 0.5730, Recall: 0.2948, F1 Score: 0.3893\n",
      "Label 'I-PER': Precision: 0.7709, Recall: 0.5845, F1 Score: 0.6649\n",
      "Macro F1: 0.523524735566532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:09<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.3232\n",
      "Label 'O': Precision: 0.9576, Recall: 0.9773, F1 Score: 0.9673\n",
      "Label 'B-ORG': Precision: 0.3856, Recall: 0.3781, F1 Score: 0.3818\n",
      "Label 'B-LOC': Precision: 0.6454, Recall: 0.6935, F1 Score: 0.6686\n",
      "Label 'I-ORG': Precision: 0.3940, Recall: 0.5965, F1 Score: 0.4746\n",
      "Label 'I-MISC': Precision: 0.5353, Recall: 0.3728, F1 Score: 0.4395\n",
      "Label 'B-MISC': Precision: 0.6116, Recall: 0.4338, F1 Score: 0.5076\n",
      "Label 'B-PER': Precision: 0.7260, Recall: 0.4805, F1 Score: 0.5782\n",
      "Label 'I-PER': Precision: 0.8103, Recall: 0.6373, F1 Score: 0.7135\n",
      "Label 'I-LOC': Precision: 0.7134, Recall: 0.4358, F1 Score: 0.5411\n",
      "Macro F1: 0.585801503699482\n"
     ]
    }
   ],
   "source": [
    "# We first provide you with a minimum code to make the entire train/eval pipeline working\n",
    "# so you can observe how it works and what the outputs are.\n",
    "train_data, eval_data, _, _ = extract_ner_data()\n",
    "model = SimpleRNNForNER(50000, 9)\n",
    "train(model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c89SjkRklQL4"
   },
   "source": [
    "Then, you need to write some experiment code to provide at least two conclusions on what factor works, or not (negative conclusions are also valid conclusions). The conclusions do not need to be extremely extensive.\n",
    "\n",
    "The conclusions you make can be towards studying the differences of\n",
    "- 1) learning rates\n",
    "- 2) hidden dim\n",
    "- 3) different activations or more non-linear activations (e.g., in SimpleRNN)\n",
    "- 4) more hidden layers\n",
    "- 5) different weight initialization methods\n",
    "- 6) ... (anything else as long as it makes sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtveTvinlXn3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Write your own code to experiment different factors.\n",
    "You are free to change the parameter/function call/model structures in the RNN class, as long as\n",
    "- 1) It's still a vanilla RNN (not its variant such as LSTM)\n",
    "- 2) Our provided minimum running script above works, because we will use it for grading.\n",
    "- 3) You do not use any Pytorch built-in models.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:32<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.4873\n",
      "Label 'O': Precision: 0.8974, Recall: 0.9851, F1 Score: 0.9392\n",
      "Label 'B-ORG': Precision: 0.3826, Recall: 0.0984, F1 Score: 0.1566\n",
      "Label 'B-LOC': Precision: 0.4778, Recall: 0.5389, F1 Score: 0.5065\n",
      "Label 'B-MISC': Precision: 0.2778, Recall: 0.0054, F1 Score: 0.0106\n",
      "Label 'I-PER': Precision: 0.6130, Recall: 0.4836, F1 Score: 0.5406\n",
      "Label 'I-MISC': Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Label 'B-PER': Precision: 0.7496, Recall: 0.2763, F1 Score: 0.4038\n",
      "Label 'I-ORG': Precision: 0.3800, Recall: 0.0506, F1 Score: 0.0893\n",
      "Label 'I-LOC': Precision: 0.4778, Recall: 0.3346, F1 Score: 0.3936\n",
      "Macro F1: 0.33780645093102957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:37<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.3847\n",
      "Label 'O': Precision: 0.9369, Recall: 0.9782, F1 Score: 0.9571\n",
      "Label 'B-ORG': Precision: 0.3424, Recall: 0.3945, F1 Score: 0.3666\n",
      "Label 'B-LOC': Precision: 0.7333, Recall: 0.4894, F1 Score: 0.5870\n",
      "Label 'I-ORG': Precision: 0.3206, Recall: 0.3249, F1 Score: 0.3228\n",
      "Label 'B-MISC': Precision: 0.5490, Recall: 0.0911, F1 Score: 0.1563\n",
      "Label 'I-PER': Precision: 0.6594, Recall: 0.6473, F1 Score: 0.6533\n",
      "Label 'I-MISC': Precision: 0.7532, Recall: 0.1676, F1 Score: 0.2742\n",
      "Label 'B-PER': Precision: 0.5869, Recall: 0.4929, F1 Score: 0.5359\n",
      "Label 'I-LOC': Precision: 0.7109, Recall: 0.3541, F1 Score: 0.4727\n",
      "Macro F1: 0.4806479782427064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:41<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:06<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.3331\n",
      "Label 'O': Precision: 0.9518, Recall: 0.9806, F1 Score: 0.9660\n",
      "Label 'B-ORG': Precision: 0.3606, Recall: 0.5101, F1 Score: 0.4225\n",
      "Label 'B-LOC': Precision: 0.7801, Recall: 0.5117, F1 Score: 0.6180\n",
      "Label 'I-ORG': Precision: 0.3907, Recall: 0.4834, F1 Score: 0.4321\n",
      "Label 'B-MISC': Precision: 0.5083, Recall: 0.4306, F1 Score: 0.4662\n",
      "Label 'I-MISC': Precision: 0.6796, Recall: 0.4046, F1 Score: 0.5072\n",
      "Label 'B-PER': Precision: 0.7182, Recall: 0.4566, F1 Score: 0.5582\n",
      "Label 'I-PER': Precision: 0.7992, Recall: 0.5998, F1 Score: 0.6853\n",
      "Label 'I-LOC': Precision: 0.7464, Recall: 0.4008, F1 Score: 0.5215\n",
      "Macro F1: 0.5752457142193325\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data, _, _ = extract_ner_data()\n",
    "model = SimpleRNNForNER(50000, 9, dropout=0.4)\n",
    "train(model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:13<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.5008\n",
      "Label 'O': Precision: 0.8957, Recall: 0.9781, F1 Score: 0.9351\n",
      "Label 'B-ORG': Precision: 0.2818, Recall: 0.3236, F1 Score: 0.3013\n",
      "Label 'I-ORG': Precision: 0.2312, Recall: 0.1065, F1 Score: 0.1459\n",
      "Label 'B-LOC': Precision: 0.5330, Recall: 0.4132, F1 Score: 0.4655\n",
      "Label 'B-MISC': Precision: 0.2857, Recall: 0.0043, F1 Score: 0.0085\n",
      "Label 'I-MISC': Precision: 0.2500, Recall: 0.0029, F1 Score: 0.0057\n",
      "Label 'B-PER': Precision: 0.7711, Recall: 0.2524, F1 Score: 0.3804\n",
      "Label 'I-PER': Precision: 0.8004, Recall: 0.2946, F1 Score: 0.4306\n",
      "Label 'I-LOC': Precision: 0.3682, Recall: 0.3696, F1 Score: 0.3689\n",
      "Macro F1: 0.33799470186830544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:24<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:06<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.3968\n",
      "Label 'O': Precision: 0.9141, Recall: 0.9883, F1 Score: 0.9498\n",
      "Label 'B-ORG': Precision: 0.4037, Recall: 0.2409, F1 Score: 0.3017\n",
      "Label 'I-ORG': Precision: 0.5154, Recall: 0.1784, F1 Score: 0.2651\n",
      "Label 'B-LOC': Precision: 0.6116, Recall: 0.5966, F1 Score: 0.6040\n",
      "Label 'B-MISC': Precision: 0.4643, Recall: 0.1410, F1 Score: 0.2163\n",
      "Label 'I-MISC': Precision: 0.7463, Recall: 0.2890, F1 Score: 0.4167\n",
      "Label 'B-PER': Precision: 0.7661, Recall: 0.3876, F1 Score: 0.5148\n",
      "Label 'I-PER': Precision: 0.8231, Recall: 0.4912, F1 Score: 0.6152\n",
      "Label 'I-LOC': Precision: 0.6772, Recall: 0.4163, F1 Score: 0.5157\n",
      "Macro F1: 0.48880485782314537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [02:21<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:05<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.3373\n",
      "Label 'O': Precision: 0.9428, Recall: 0.9828, F1 Score: 0.9624\n",
      "Label 'B-ORG': Precision: 0.4505, Recall: 0.3669, F1 Score: 0.4044\n",
      "Label 'B-LOC': Precision: 0.6696, Recall: 0.6146, F1 Score: 0.6409\n",
      "Label 'I-MISC': Precision: 0.7107, Recall: 0.4046, F1 Score: 0.5157\n",
      "Label 'B-MISC': Precision: 0.4784, Recall: 0.3970, F1 Score: 0.4339\n",
      "Label 'I-ORG': Precision: 0.4893, Recall: 0.4554, F1 Score: 0.4717\n",
      "Label 'B-PER': Precision: 0.7286, Recall: 0.4533, F1 Score: 0.5589\n",
      "Label 'I-PER': Precision: 0.7780, Recall: 0.6060, F1 Score: 0.6813\n",
      "Label 'I-LOC': Precision: 0.7312, Recall: 0.5292, F1 Score: 0.6140\n",
      "Macro F1: 0.5870265223279222\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data, _, _ = extract_ner_data()\n",
    "model = SimpleRNNForNER(50000, 9, dropout=0.1)\n",
    "train(model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden dim testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [01:32<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:04<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.5196\n",
      "Label 'O': Precision: 0.8792, Recall: 0.9873, F1 Score: 0.9301\n",
      "Label 'B-ORG': Precision: 0.2780, Recall: 0.1999, F1 Score: 0.2325\n",
      "Label 'B-LOC': Precision: 0.6654, Recall: 0.2771, F1 Score: 0.3912\n",
      "Label 'B-MISC': Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Label 'I-PER': Precision: 0.5512, Recall: 0.2800, F1 Score: 0.3714\n",
      "Label 'I-MISC': Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Label 'B-PER': Precision: 0.6081, Recall: 0.2611, F1 Score: 0.3654\n",
      "Label 'I-LOC': Precision: 0.5865, Recall: 0.2374, F1 Score: 0.3380\n",
      "Label 'I-ORG': Precision: 0.2857, Recall: 0.0213, F1 Score: 0.0397\n",
      "Macro F1: 0.2964748819088397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [01:35<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:04<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.4095\n",
      "Label 'O': Precision: 0.9281, Recall: 0.9806, F1 Score: 0.9536\n",
      "Label 'B-ORG': Precision: 0.3092, Recall: 0.2893, F1 Score: 0.2989\n",
      "Label 'B-LOC': Precision: 0.5874, Recall: 0.5961, F1 Score: 0.5917\n",
      "Label 'B-MISC': Precision: 0.5167, Recall: 0.0336, F1 Score: 0.0631\n",
      "Label 'I-ORG': Precision: 0.3135, Recall: 0.2876, F1 Score: 0.3000\n",
      "Label 'I-MISC': Precision: 0.8000, Recall: 0.1272, F1 Score: 0.2195\n",
      "Label 'B-PER': Precision: 0.6356, Recall: 0.3882, F1 Score: 0.4820\n",
      "Label 'I-PER': Precision: 0.7070, Recall: 0.5318, F1 Score: 0.6070\n",
      "Label 'I-LOC': Precision: 0.6346, Recall: 0.3852, F1 Score: 0.4794\n",
      "Macro F1: 0.4439180407869578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [01:35<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.3042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:04<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.3450\n",
      "Label 'O': Precision: 0.9514, Recall: 0.9788, F1 Score: 0.9649\n",
      "Label 'B-ORG': Precision: 0.3627, Recall: 0.4079, F1 Score: 0.3840\n",
      "Label 'B-LOC': Precision: 0.6826, Recall: 0.6462, F1 Score: 0.6639\n",
      "Label 'B-PER': Precision: 0.6869, Recall: 0.4609, F1 Score: 0.5517\n",
      "Label 'B-MISC': Precision: 0.4490, Recall: 0.2386, F1 Score: 0.3116\n",
      "Label 'I-LOC': Precision: 0.5556, Recall: 0.4475, F1 Score: 0.4957\n",
      "Label 'I-MISC': Precision: 0.5370, Recall: 0.2514, F1 Score: 0.3425\n",
      "Label 'I-PER': Precision: 0.7751, Recall: 0.6327, F1 Score: 0.6967\n",
      "Label 'I-ORG': Precision: 0.3472, Recall: 0.4447, F1 Score: 0.3900\n",
      "Macro F1: 0.5334315240717614\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data, _, _ = extract_ner_data()\n",
    "model = SimpleRNNForNER(50000, 9, hidden_dim=64)\n",
    "train(model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [05:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:09<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.4530\n",
      "Label 'O': Precision: 0.9028, Recall: 0.9805, F1 Score: 0.9401\n",
      "Label 'B-ORG': Precision: 0.3282, Recall: 0.2677, F1 Score: 0.2949\n",
      "Label 'B-LOC': Precision: 0.5772, Recall: 0.5024, F1 Score: 0.5373\n",
      "Label 'I-ORG': Precision: 0.3955, Recall: 0.0932, F1 Score: 0.1509\n",
      "Label 'I-LOC': Precision: 0.7059, Recall: 0.4202, F1 Score: 0.5268\n",
      "Label 'B-MISC': Precision: 0.3460, Recall: 0.0987, F1 Score: 0.1536\n",
      "Label 'I-MISC': Precision: 0.7979, Recall: 0.2168, F1 Score: 0.3409\n",
      "Label 'B-PER': Precision: 0.7766, Recall: 0.3208, F1 Score: 0.4541\n",
      "Label 'I-PER': Precision: 0.7506, Recall: 0.4491, F1 Score: 0.5620\n",
      "Macro F1: 0.4400530260381648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [05:32<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:09<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.3676\n",
      "Label 'O': Precision: 0.9307, Recall: 0.9827, F1 Score: 0.9560\n",
      "Label 'B-ORG': Precision: 0.4486, Recall: 0.1984, F1 Score: 0.2751\n",
      "Label 'B-LOC': Precision: 0.5940, Recall: 0.6347, F1 Score: 0.6137\n",
      "Label 'I-ORG': Precision: 0.5040, Recall: 0.2517, F1 Score: 0.3357\n",
      "Label 'B-MISC': Precision: 0.4593, Recall: 0.3059, F1 Score: 0.3672\n",
      "Label 'I-MISC': Precision: 0.8456, Recall: 0.3324, F1 Score: 0.4772\n",
      "Label 'B-PER': Precision: 0.7403, Recall: 0.4131, F1 Score: 0.5303\n",
      "Label 'I-PER': Precision: 0.6548, Recall: 0.6764, F1 Score: 0.6654\n",
      "Label 'I-LOC': Precision: 0.7452, Recall: 0.4553, F1 Score: 0.5652\n",
      "Macro F1: 0.5317509963674997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|████████████████████████████████████████████████████████████| 439/439 [05:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation: 100%|██████████████████████████████████████████████████████████| 102/102 [00:09<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.3191\n",
      "Label 'O': Precision: 0.9560, Recall: 0.9762, F1 Score: 0.9660\n",
      "Label 'B-ORG': Precision: 0.4877, Recall: 0.3400, F1 Score: 0.4007\n",
      "Label 'B-LOC': Precision: 0.5822, Recall: 0.7496, F1 Score: 0.6554\n",
      "Label 'I-ORG': Precision: 0.4928, Recall: 0.4554, F1 Score: 0.4734\n",
      "Label 'I-LOC': Precision: 0.7402, Recall: 0.5875, F1 Score: 0.6551\n",
      "Label 'B-MISC': Precision: 0.6105, Recall: 0.3774, F1 Score: 0.4665\n",
      "Label 'I-MISC': Precision: 0.7812, Recall: 0.4335, F1 Score: 0.5576\n",
      "Label 'B-PER': Precision: 0.7056, Recall: 0.5114, F1 Score: 0.5930\n",
      "Label 'I-PER': Precision: 0.6908, Recall: 0.7437, F1 Score: 0.7163\n",
      "Macro F1: 0.609331173843403\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data, _, _ = extract_ner_data()\n",
    "model = SimpleRNNForNER(50000, 9, hidden_dim=256)\n",
    "train(model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJL6F0xInCC0"
   },
   "source": [
    "## Submission (Task 1&2)\n",
    "You will submit this Notebook with intermediate running outputs. If you made changes to the RNN classes, you can keep it as is, no need to revert back to a vanilla version.\n",
    "## Submission (Task 2)\n",
    "Write here on what are the two conclusions you found above? ~50 words (within 100) for each conclusion. Make sure your conclusion involves quantitative evidences from the Macro F1 score.\n",
    "\n",
    "Basically I checked the effect hidden layers and dropout parameters have on the model's performance. In the modified architecture I implemented a single dropout before the final linear layer. The reasoning is to control the complexity of the model and attenuating possible overfitting problems. In our case, though, model is not too complex and increasing the dropout results in a slight drop in performance. Hidden layers, in their turn, increase the complexity of the model and, as results tell us, increase the performance by a considerate amount (~ 7% difference). The model should perform better with complexity increase."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
